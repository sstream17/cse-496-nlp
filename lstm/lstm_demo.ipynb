{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import Layer, LSTM, Bidirectional, Dense, Input, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import json\n",
    "\n",
    "from ArbitraryLearningRates import LearningRateCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(Layer):\n",
    "    \n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention,self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x,self.W)+self.b)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "bag = []\n",
    "if os.path.exists('all_lyrics.txt'):\n",
    "    # Read all data from one file\n",
    "    with open(\"all_lyrics.txt\") as f:\n",
    "        bag = f.readlines()\n",
    "else:\n",
    "    # Compile data from files\n",
    "    for filename in os.listdir(r\"lyrics\"):\n",
    "        with open(\"lyrics/\" + filename, encoding=\"cp1252\") as f:\n",
    "            data = f.read().replace('–', '').replace('|','') + '}'#.encode('utf8')\n",
    "            \n",
    "            # Parse data into structure\n",
    "            # Returns a big dictionary\n",
    "            dic = json.loads(data)\n",
    "            \n",
    "            # Combine the entries into a bag-of-lines\n",
    "            for line in dic[\"Lyrics\"][0]:\n",
    "                # Don't want song structure or empty lines\n",
    "                if line != '':\n",
    "                    if line[0] != '[':\n",
    "                        bag.append(line + ' ENDTOKEN')\n",
    "                        #bag.append(line)\n",
    "    with open(\"all_lyrics.txt\", 'w') as f:\n",
    "        for line in bag:\n",
    "            f.write(line + '\\r\\n')\n",
    "    \n",
    "num_lines = len(bag)\n",
    "\n",
    "# Tokenize the bag\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', split=' ')\n",
    "tokenizer.fit_on_texts(bag)\n",
    "\n",
    "# Mapping from words to integers in the vocabulary\n",
    "word_to_integer = tokenizer.word_index\n",
    "\n",
    "# Size of the vocabulary\n",
    "num_words = len(word_to_integer.keys())+1\n",
    "\n",
    "# Create reverse mapping from integers to words\n",
    "integer_to_word = {val: key for key, val in word_to_integer.items()}\n",
    "\n",
    "# Get the token-by-token data\n",
    "input_sequences = []\n",
    "for line in bag:\n",
    "    # Get the integer sequence representation of the line\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(0, len(token_list)):\n",
    "        # Take progressively larger slices of the input sequence (maybe remove?)\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        \n",
    "        # Add each slice to the input_sequences\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        \n",
    "# We need to pad each sequence to be the maximum length\n",
    "max_sequence_length = max([len(x) for x in input_sequences])-1\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length+1, padding = 'pre'))\n",
    "\n",
    "# xs is the sequence except the last word\n",
    "# ys is the last word in the sequence\n",
    "# This gives us sequences matched to their next word, which hopefully the model can learn\n",
    "xs, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=num_words)\n",
    "\n",
    "dim = max_sequence_length*2\n",
    "\n",
    "# Layer descriptions\n",
    "x_in = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "emb = tf.keras.layers.Embedding(num_words, dim, input_length=max_sequence_length)\n",
    "x_emb = emb(x_in)\n",
    "lstm1 = LSTM(dim, return_sequences=True)\n",
    "x_seq_enc = lstm1(x_emb)\n",
    "x_att = attention(return_sequences=True)(x_emb)\n",
    "x_2 = Bidirectional(LSTM(dim, return_sequences=True))(x_att)\n",
    "x_3 = Bidirectional(LSTM(dim, return_sequences=True))(x_2)\n",
    "x_4 = Bidirectional(LSTM(dim))(x_3)\n",
    "d = Dense(num_words, activation='softmax')(x_4)\n",
    "out = Dropout(0.1)(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mister for you\n",
      "letting you see through me\n",
      "fever along the way\n",
      "carried on the feeling\n",
      "ran with me i'm fading\n",
      "own up to the sin\n",
      "sail up high i'm strong enough\n",
      "must've took a while just to find the words\n",
      "cinderella\n",
      "needed downtown for all to blame\n",
      "mouth the side of a damn friend dream\n",
      "fifties half a rock star and come on\n",
      "cloud in my mind\n",
      "preserved of the deep but this\n",
      "forces from the edge\n",
      "responds easy home\n",
      "q a hundred way you bring inside girl\n",
      "defined\n",
      "sorry i canâ€™t always find the words to say\n",
      "prison gates won't open up for me\n"
     ]
    }
   ],
   "source": [
    "# Set up how our generated lyrics will look\n",
    "next_words = max_sequence_length\n",
    "\n",
    "for j in range(0,20):\n",
    "    seed = integer_to_word[random.randint(1,num_words)]\n",
    "    for i in range(next_words):\n",
    "        # Returns a list of lists, we just want one (the only)\n",
    "        token_list = tokenizer.texts_to_sequences([seed])[0]\n",
    "        \n",
    "        # Pad the list\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_length, padding=\"pre\")\n",
    "        \n",
    "        # Index of maximum in predictions\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        \n",
    "        # find word corresponding to the maximal index\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        if output_word == \"endtoken\":\n",
    "            break\n",
    "        seed = seed + \" \" + output_word\n",
    "\n",
    "    print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
